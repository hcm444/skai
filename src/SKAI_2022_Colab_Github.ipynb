{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/google-research/skai/blob/skai-colab/src/SKAI_2022_Colab_Github.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qxZ4Nbz5x6Bw"
      },
      "source": [
        "# **The SKAI isn‚Äôt the limit üöÄ**\n",
        "***Assessing Post-Disaster Damage üèöÔ∏è from Satellite Imagery üõ∞Ô∏è using Semi-Supervised Learning Techniques üìî***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WKWAgrAFl6U1"
      },
      "source": [
        "*by Amine Baha, WFP Innovation Accelerator, 04th June 2022*"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Intro üèπ"
      ],
      "metadata": {
        "id": "A1IfnanckHeo"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zAc_6ag50kyU"
      },
      "source": [
        "WFP partnered with Google Research to set up **SKAI**, a humanitarian response mapping solution powered by artificial intelligence ‚Äî an approach that combines statistical methods, data and modern computing techniques to automate specific tasks. SKAI assesses damage to buildings by applying computer vision ‚Äî computer algorithms that can interpret information extracted from visual materials such as, in this case, **satellite images of areas impacted by conflict, climate events, or other disasters**.\n",
        "\n",
        "![Skai Logo](https://storage.googleapis.com/skai-public/skai_logo.png)\n",
        "\n",
        "The type of machine learning used in SKAI, learns from a small number of labeled and a large number of unlabeled images of affected buildings. SKAI uses a ***semi-supervised learning technique*** that reduces the required number of labeled examples by an order of magnitude. As such, SKAI models typically *only need a couple hundred labeled examples* to achieve high accuracy, significantly improving the speed at which accurate results can be obtained.\n",
        "\n",
        "Google Research presented this novel application of semi-supervised learning (SSL) to train models for damage assessment with a minimal amount of labeled data and large amount of unlabeled data in [June 2020](https://ai.googleblog.com/2020/06/machine-learning-based-damage.html). Using the state-of-the-art methods including [MixMatch](https://arxiv.org/abs/1905.02249) and [FixMatch](https://arxiv.org/abs/2001.07685), they compare the performance with supervised baseline for the 2010 Haiti earthquake, 2017 Santa Rosa wildfire, and 2016 armed conflict in Syria.\n",
        "\n",
        "![SSL Approach](https://storage.googleapis.com/skai-public/ssl_diagram.png)\n",
        "\n",
        "The [paper](https://arxiv.org/abs/2011.14004) published by *Jihyeon Lee, Joseph Z. Xu, Kihyuk Sohn, Wenhan Lu, David Berthelot, Izzeddin Gur, Pranav Khaitan, Ke-Wei, Huang, Kyriacos Koupparis, Bernhard Kowatsch* shows how models trained with SSL methods can reach fully supervised performance despite using only a fraction of labeled data.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Please run this cell first!\n",
        "\n",
        "import base64\n",
        "import collections\n",
        "import datetime\n",
        "import io\n",
        "import json\n",
        "import os\n",
        "import re\n",
        "import subprocess\n",
        "import time\n",
        "\n",
        "import ee\n",
        "import folium\n",
        "import ipyplot\n",
        "import IPython.display\n",
        "import pandas as pd\n",
        "import pexpect\n",
        "import pprint\n",
        "import pyproj\n",
        "import pytz\n",
        "import requests\n",
        "import smtplib\n",
        "import ssl\n",
        "import tensorflow as tf\n",
        "\n",
        "from os import path\n",
        "from google.appengine.api import mail\n",
        "from google.cloud import monitoring_v3\n",
        "from IPython.display import display, HTML, Javascript\n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "\n",
        "def launch_pexpect_process(venv_dir, skai_dir, script, arguments, use_pexpect):\n",
        "  flags_str = ' '.join(f\"--{f}='{v}'\" for f, v in arguments.items())\n",
        "  commands = '; '.join([\n",
        "      f'set -e',\n",
        "      f'source {venv_dir}/bin/activate',\n",
        "      f'export GOOGLE_APPLICATION_CREDENTIALS=/root/service-account-private-key.json',\n",
        "      f'python {skai_dir}/src/{script} {flags_str}'])\n",
        "  sh_command = f'bash -c \"{commands}\" | tee /tmp/output.txt'\n",
        "  if use_pexpect:\n",
        "    return pexpect.spawn(sh_command)\n",
        "  else:\n",
        "    with open('/tmp/shell_command.sh', 'w') as f:\n",
        "      f.write(commands)\n",
        "    !bash \"/tmp/shell_command.sh\" | tee /tmp/output.txt\n",
        "\n",
        "def make_gcp_http_request(url):\n",
        "  variable=!(gcloud auth print-access-token)\n",
        "  token=variable[0]\n",
        "  response = requests.get(url=url, headers = {\"Authorization\": \"Bearer {token}\".format(token=token)})\n",
        "  if not response.ok:\n",
        "    response.raise_for_status()\n",
        "  return response.json()\n",
        "\n",
        "def bucket_exists(project, bucket_name):\n",
        "  url = f'https://storage.googleapis.com/storage/v1/b?project={project}'\n",
        "  data = make_gcp_http_request(url)\n",
        "  buckets = [item['name'] for item in data['items'] if item['kind'] == 'storage#bucket']\n",
        "  return (bucket_name in buckets)\n",
        "\n",
        "def create_bucket(project, location, bucket_name):\n",
        "  %shell gsutil mb -p $project -l $location -b on gs://{bucket_name}\n",
        "\n",
        "def progress(value, max=100):\n",
        "  css = \"\"\"\n",
        "        <style>\n",
        "          progress {\n",
        "            border-radius: 7px;\n",
        "            box-shadow: 1px 1px 2px rgba(0, 0, 0, 0.5) inset;\n",
        "            width: 80%;\n",
        "            height: 30px;\n",
        "            display: block;\n",
        "          }\n",
        "          progress::-webkit-progress-bar {\n",
        "            background-color: rgba(237, 237, 237, 0);\n",
        "            border-radius: 7px;\n",
        "          }\n",
        "          progress::-webkit-progress-value {\n",
        "            background-color: green;\n",
        "            border-radius: 7px;\n",
        "            box-shadow: 1px 1px 1px rgba(0, 0, 0, 0.1) inset;\n",
        "          }\n",
        "        </style>\n",
        "        \"\"\"\n",
        "  html = \"\"\"\n",
        "          <progress\n",
        "              value='{value}'\n",
        "              max='{max}'\n",
        "          >\n",
        "            {value}%\n",
        "          </progress>\n",
        "        \"\"\".format(value=value, max=max)\n",
        "  return HTML(css + html)\n"
      ],
      "metadata": {
        "id": "5nPcG8XJnT91",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nihwE_UZFilS"
      },
      "source": [
        "## Notebook Setup üìì"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Specify the variables to set your damage assessment project and press play:"
      ],
      "metadata": {
        "id": "uTLBbr2j911p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#############################################\n",
        "### INITIAL SETTING - PROJECT DESCRIPTION ###\n",
        "#############################################\n",
        "#@markdown ---\n",
        "#@markdown Please enter here the parameters for your **project desciption**\n",
        "\n",
        "#@markdown ---\n",
        "Disaster = 'Cyclone' #@param [\"Cyclone\", \"Earthquake\", \"Tsunami\", \"Flood\", \"Eruption\", \"Tornado\", \"Wind\", \"Wildfire\", \"Landslide\", \"Conflict\"]\n",
        "Year =  #@param {type:\"integer\"}\n",
        "Month =   #@param {type:\"integer\"}\n",
        "Name = '' #@param {type:\"string\"}\n",
        "Country = '' #@param {type:\"string\"}\n",
        "Organisation = '' #@param {type:\"string\"}\n",
        "Author = '' #@param {type:\"string\"}\n",
        "Run = '' #@param {type:\"string\"}\n",
        "\n",
        "Project_description= f\"{Organisation}-{Disaster}-{Name}-{Country}-{Year}{Month}_{Run}\".lower()\n",
        "print(f\"\\nYour project description: {Project_description}\")\n",
        "\n",
        "currentDateTime = datetime.datetime.now()\n",
        "date = currentDateTime.date()\n",
        "timestamp=currentDateTime.strftime('%Y%m%d%H%M%S')\n",
        "year = date.strftime(\"%Y\")\n",
        "\n",
        "Tool=\"Skai\"\n",
        "Env=\"Test\"\n",
        "Version=\"102\"\n",
        "\n",
        "root_filesys='/content'\n",
        "GD_DIRECTORY = f\"{Project_description}\".lower()\n",
        "\n",
        "#############################################\n",
        "### CLOUD SETTING - PROJECT CONFIGURATION ###\n",
        "#############################################\n",
        "#markdown ---\n",
        "#markdown Please enter here the project and location names you want to use in your **google cloud platform account**\n",
        "\n",
        "#markdown ---\n",
        "GCP_PROJECT = \"skai-2022\" #param {type:\"string\"}\n",
        "GCP_LOCATION = \"europe-west1\" #param {type:\"string\"}\n",
        "GCP_LOCATION_LABELING=GCP_LOCATION\n",
        "if \"europe-\" in GCP_LOCATION :\n",
        "  GCP_LOCATION_LABELING= \"europe-west4\"\n",
        "  if GCP_LOCATION!= \"europe-west1\" :\n",
        "    GCP_LOCATION= \"europe-west1\"\n",
        "    print(f\"\\nLocation region has been changed to {GCP_LOCATION} (Vertex AI features availability) \")\n",
        "if \"us-\" in GCP_LOCATION :\n",
        "  GCP_LOCATION_LABELING= \"us-central1\"\n",
        "  if GCP_LOCATION!= \"us-central1\" :\n",
        "    GCP_LOCATION= \"us-central1\"\n",
        "    print(f\"\\nLocation region has been changed to {GCP_LOCATION} (Vertex AI features availability) \")\n",
        "\n",
        "url='https://cloudresourcemanager.googleapis.com/v1/projects/{}'.format(GCP_PROJECT)\n",
        "data = make_gcp_http_request(url)\n",
        "GCP_PROJECT_ID=int(data['projectNumber'])\n",
        "\n",
        "GCP_BUCKET = f\"{Tool}{year}-Bucket-{Env}{Version}_{Author}\".lower()\n",
        "if not bucket_exists(GCP_PROJECT, GCP_BUCKET):\n",
        "  create_bucket(GCP_PROJECT, GCP_BUCKET)\n",
        "\n",
        "print(f\"\\nYour project bucket in Google Cloud: {GCP_BUCKET} \\nhttps://console.cloud.google.com/storage/browser/{GCP_BUCKET}\")\n",
        "\n",
        "pathgcp_outputdir=os.path.join(GCP_BUCKET,GD_DIRECTORY)\n",
        "\n",
        "emailgcp_serviceaccount = 'skai-colab@skai-2022.iam.gserviceaccount.com'\n",
        "\n",
        "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = '/root/service-account-private-key.json'\n",
        "\n",
        "#############################################\n",
        "### CODE SETTING - ENVIRONMENT ACTIVATION ###\n",
        "#############################################\n",
        "pathsys_venv=os.path.join(root_filesys,'skai-env')\n",
        "pathsys_actenv=os.path.join(pathsys_venv, 'bin/activate')\n",
        "\n",
        "pathsys_skai=os.path.join(root_filesys, 'skai-src')\n",
        "\n",
        "#########################################\n",
        "### IMAGE SETTING - FILE & DIRECTORY ###\n",
        "#########################################\n",
        "#@markdown ---\n",
        "#@markdown Please enter the path to the files of pre and post disaster satellite images and area of interest:\n",
        "\n",
        "#@markdown ---\n",
        "FILE_IMAGE_BEFORE = '' #@param {type:\"string\"}\n",
        "FILE_IMAGE_AFTER = '' #@param {type:\"string\"}\n",
        "FILE_IMAGE_AOI = '' #@param {type:\"string\"}\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown Choose where to get building footprints from:\n",
        "BUILDING_DETECTION_METHOD = \"open_street_map\" #@param [\"open_street_map\", \"file\"]\n",
        "#@markdown If you chose \"file\", please enter path to CSV file here:\n",
        "BUILDINGS_CSV = '' #@param {type:\"string\"}\n",
        "\n",
        "pathgcp_imagebefore=FILE_IMAGE_BEFORE.replace('gs://','')\n",
        "pathgcp_imageafter=FILE_IMAGE_AFTER.replace('gs://','')\n",
        "pathgcp_aoi=FILE_IMAGE_AOI.replace('gs://','')\n",
        "\n",
        "#########################################\n",
        "### EXAMPLE SETTING - CLOUD DIRECTORY ###\n",
        "#########################################\n",
        "pathgcp_examples=os.path.join(pathgcp_outputdir,'examples')\n",
        "pathgcp_importfile=os.path.join(pathgcp_examples,'labeling_images/import_file.csv')\n",
        "\n",
        "###########################################\n",
        "### LABELING SETTING - EMAIL PARAMETERS ###\n",
        "###########################################\n",
        "#@markdown ---\n",
        "#@markdown Provide email addresses for all individuals that will help with labeling images, separated by commas.\n",
        "#@markdown Emails of the labelers need to be linked to a google account.\n",
        "\n",
        "#@markdown ---\n",
        "EMAIL_MANAGER = '' #@param {type:\"string\"}\n",
        "EMAIL_ANNOTATORS = '' #@param {type:\"string\"}\n",
        "\n",
        "if EMAIL_MANAGER.strip() in EMAIL_ANNOTATORS:\n",
        "  EMAIL_ANNOTATORS.replace(EMAIL_MANAGER.strip(), '')\n",
        "GCP_LABELER_EMAIL = [EMAIL_MANAGER.strip()] + [email.strip() for email in EMAIL_ANNOTATORS.split(',')]\n",
        "GCP_LABELER_EMAIL = ','.join(GCP_LABELER_EMAIL)\n",
        "\n",
        "################################################\n",
        "### DATASET SETTING - FILE & CLOUD DIRECTORY ###\n",
        "################################################\n",
        "pathgcp_temp=os.path.join(pathgcp_outputdir,'temp')\n",
        "pathgcp_unlabeled=os.path.join(pathgcp_examples,'unlabeled/*.tfrecord')\n",
        "\n",
        "pathgcp_trainset=os.path.join(pathgcp_examples,'labeled_train_examples.tfrecord')\n",
        "pathgcp_testset=os.path.join(pathgcp_examples,'labeled_test_examples.tfrecord')\n",
        "\n",
        "#######################################\n",
        "### MODEL SETTING - FILE & DIRECTORY ##\n",
        "#######################################\n",
        "pathsys_runjobs=os.path.join(root_filesys,'run_jobs')\n",
        "if not os.path.exists(pathsys_runjobs):\n",
        "  os.mkdir(pathsys_runjobs)\n",
        "\n",
        "pathgcp_models=os.path.join(pathgcp_outputdir,'models')\n"
      ],
      "metadata": {
        "id": "dDtCZ5QvYBom",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Optimize images for cloud\n",
        "\n",
        "pathsys_images=os.path.join(root_filesys,'images')\n",
        "pathsys_imagesfolder=os.path.join(pathsys_images,f\"{Author}_{GD_DIRECTORY}\")\n",
        "if path.exists(pathsys_images) == False:\n",
        "  os.mkdir(pathsys_images)\n",
        "  os.mkdir(pathsys_imagesfolder)\n",
        "else :\n",
        "  if path.exists(pathsys_imagesfolder) == False:\n",
        "    os.mkdir(pathsys_imagesfolder)\n",
        "\n",
        "temp_dir_image = os.path.join(pathsys_imagesfolder, 'tmp')\n",
        "validator_file = os.path.join(temp_dir_image, 'validate_cog.py')\n",
        "temp_out_file = os.path.join(temp_dir_image, 'temp_out_file.txt')\n",
        "\n",
        "pre_image = os.path.join(pathsys_imagesfolder, FILE_IMAGE_BEFORE.split('/')[-1])\n",
        "pre_image_copy = os.path.join(pathsys_imagesfolder, FILE_IMAGE_BEFORE.split('/')[-1].split('.tif')[0] + '_copy.tif')\n",
        "post_image = os.path.join(pathsys_imagesfolder, FILE_IMAGE_AFTER.split('/')[-1])\n",
        "post_image_copy = os.path.join(pathsys_imagesfolder, FILE_IMAGE_AFTER.split('/')[-1].split('.tif')[0] + '_copy.tif')\n",
        "\n",
        "pathgcp_images=os.path.join(pathgcp_outputdir,'images')\n",
        "\n",
        "!gsutil -m cp gs://{pathgcp_images}/* {pathsys_imagesfolder}/ \n",
        "\n",
        "def write_optimize_images_launch_script(**args):\n",
        "  submission_ending='''\n",
        "mkdir -p {temp_dir_image}\n",
        "curl -s 'https://raw.githubusercontent.com/OSGeo/gdal/master/swig/python/gdal-utils/osgeo_utils/samples/validate_cloud_optimized_geotiff.py' > {validator_file}\n",
        "\n",
        "python3.6 {validator_file} {pre_image} | tee {temp_out_file}\n",
        "if grep -q 'NOT a valid' {temp_out_file}; then\n",
        "  cp {pre_image} {pre_image_copy} \n",
        "  echo 'Converting pre_disaster image to COG...'\n",
        "  gdaladdo -r average {pre_image_copy} 2 4 8 16\n",
        "  gdal_translate {pre_image_copy} {pre_image} -co COMPRESS=LZW -co TILED=YES \n",
        "fi\n",
        "\n",
        "python3.6 {validator_file} {post_image} | tee {temp_out_file}\n",
        "if grep -q 'NOT a valid' {temp_out_file}; then\n",
        "  cp {post_image} {post_image_copy}\n",
        "  echo 'Converting post_disaster image to COG...'\n",
        "  gdaladdo -r average {post_image_copy} 2 4 8 16\n",
        "  gdal_translate {post_image_copy} {post_image} -co COMPRESS=LZW -co TILED=YES\n",
        "fi\n",
        "\n",
        "rm -rf {temp_dir_image}'''.format(**args)  \n",
        "  \n",
        "  with open(args['path_run'], 'w+') as file:\n",
        "    file.write(submission_ending)\n",
        "\n",
        "timestamp = datetime.datetime.now().strftime('%Y%m%d%H%M%S')\n",
        "file_runjob=f'run_optimize_as_child_process_{Author}_{timestamp}_{Project_description}.sh'\n",
        "pathsys_runfile=os.path.join(pathsys_runjobs,file_runjob)\n",
        "\n",
        "generate_script_args={\n",
        "    'validator_file':validator_file,\n",
        "    'temp_out_file':temp_out_file,\n",
        "    'pre_image':pre_image,\n",
        "    'pre_image_copy':pre_image_copy,\n",
        "    'post_image':post_image,\n",
        "    'post_image_copy':post_image_copy,\n",
        "    'temp_dir_image':temp_dir_image,\n",
        "    'path_run': pathsys_runfile,\n",
        "}\n",
        "\n",
        "write_optimize_images_launch_script(**generate_script_args)\n",
        "\n",
        "!bash {pathsys_runfile}\n",
        "\n",
        "!gsutil -m cp {pre_image} {FILE_IMAGE_BEFORE}\n",
        "!gsutil -m cp {post_image} {FILE_IMAGE_AFTER}\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "Gj3r3kND9zws"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Visualize before and after images\n",
        "\n",
        "# Add custom basemaps to folium.\n",
        "basemaps = {\n",
        "    'Google Maps': folium.TileLayer(\n",
        "        tiles = 'https://mt1.google.com/vt/lyrs=m&x={x}&y={y}&z={z}',\n",
        "        attr = 'Google',\n",
        "        name = 'Google Maps',\n",
        "        overlay = True,\n",
        "        control = True\n",
        "    ),\n",
        "    'Google Satellite': folium.TileLayer(\n",
        "        tiles = 'https://mt1.google.com/vt/lyrs=s&x={x}&y={y}&z={z}',\n",
        "        attr = 'Google',\n",
        "        name = 'Google Satellite',\n",
        "        overlay = True,\n",
        "        control = True\n",
        "    ),\n",
        "    'Google Terrain': folium.TileLayer(\n",
        "        tiles = 'https://mt1.google.com/vt/lyrs=p&x={x}&y={y}&z={z}',\n",
        "        attr = 'Google',\n",
        "        name = 'Google Terrain',\n",
        "        overlay = True,\n",
        "        control = True\n",
        "    ),\n",
        "    'Google Satellite Hybrid': folium.TileLayer(\n",
        "        tiles = 'https://mt1.google.com/vt/lyrs=y&x={x}&y={y}&z={z}',\n",
        "        attr = 'Google',\n",
        "        name = 'Google Satellite',\n",
        "        overlay = True,\n",
        "        control = True\n",
        "    ),\n",
        "    'Esri Satellite': folium.TileLayer(\n",
        "        tiles = 'https://server.arcgisonline.com/ArcGIS/rest/services/World_Imagery/MapServer/tile/{z}/{y}/{x}',\n",
        "        attr = 'Esri',\n",
        "        name = 'Esri Satellite',\n",
        "        overlay = True,\n",
        "        control = True\n",
        "    )\n",
        "}\n",
        "\n",
        "def create_folium_map_with_images():\n",
        "  # Load before image and get latitude/longitude of map center.\n",
        "  before_image_path = 'gs://'+pathgcp_imagebefore\n",
        "  before_map = ee.Image.loadGeoTIFF(before_image_path)\n",
        "  before_map_id_dict = before_map.getMapId()\n",
        "  x = before_map.getInfo()['bands'][0]['crs_transform'][2]\n",
        "  y = before_map.getInfo()['bands'][0]['crs_transform'][-1]\n",
        "  dim_x, dim_y = before_map.getInfo()['bands'][0]['dimensions']\n",
        "  crs = before_map.getInfo()['bands'][0]['crs'].split(':')[-1]\n",
        "  proj = pyproj.Transformer.from_crs(int(crs), 4326, always_xy=True)\n",
        "  lon, lat = proj.transform(x + int(dim_x / 4), y - int(dim_y / 4))\n",
        "\n",
        "  # Create a folium map object. Location is latitude, longitude.\n",
        "  my_map = folium.Map(location=[lat, lon], zoom_start=12, max_zoom=25)\n",
        "\n",
        "  # Add before and after disaster imagery.\n",
        "  folium.raster_layers.TileLayer(\n",
        "      tiles=before_map_id_dict['tile_fetcher'].url_format,\n",
        "      attr='COG',\n",
        "      name = 'Pre-Disaster Imagery',\n",
        "      overlay = True,\n",
        "      control = True,\n",
        "      max_zoom = 25,\n",
        "    ).add_to(my_map)\n",
        "\n",
        "  after_image_path = 'gs://'+pathgcp_imageafter\n",
        "  after_map_id_dict = ee.Image.loadGeoTIFF(after_image_path).getMapId()\n",
        "  folium.raster_layers.TileLayer(\n",
        "      tiles=after_map_id_dict['tile_fetcher'].url_format,\n",
        "      attr='COG',\n",
        "      name = 'Post-Disaster Imagery',\n",
        "      overlay = True,\n",
        "      control = True,\n",
        "      max_zoom = 25,\n",
        "    ).add_to(my_map)\n",
        "\n",
        "  my_map.add_child(folium.LayerControl())\n",
        "  IPython.display.display(my_map)\n",
        "\n",
        "display(Javascript(\"google.colab.output.resizeIframeToContent()\"))\n",
        "\n",
        "\n",
        "# Prepare credentials for map visualization.\n",
        "service_account = 'skai-colab@skai-2022.iam.gserviceaccount.com'\n",
        "credentials = ee.ServiceAccountCredentials(\n",
        "    service_account, '/root/service-account-private-key.json')\n",
        "ee.Initialize(credentials)\n",
        "\n",
        "create_folium_map_with_images()\n"
      ],
      "metadata": {
        "id": "2rqwIPUfjq4H",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Esh2w1R5AAJy"
      },
      "source": [
        "## Data labeling üë∑\n",
        "\n",
        "Create examples of buildings images before and after the disaster and classify them as either undamaged, possibly damaged, damaged/destroyed, or bad example (e.g., cloud cover etc.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jaIc0OFAABB3"
      },
      "source": [
        "First, generate the building images, this task should take about 30 minutes."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Generate Examples\n",
        "\n",
        "## CLASS DEFINITION\n",
        "\n",
        "class DataflowMetricFetcher:\n",
        "  def __init__(self, project_id: str, job_name: str, metric_name: str):\n",
        "    self._client = monitoring_v3.MetricServiceClient()\n",
        "    self._project_id = project_id\n",
        "    self._job_name = job_name\n",
        "    self._metric_name = metric_name\n",
        "    self._filter = self.make_filter()\n",
        "\n",
        "  def make_filter(self):\n",
        "    conditions = [\n",
        "        'resource.type = \"dataflow_job\"',\n",
        "        f'resource.labels.project_id = \"{self._project_id}\"',\n",
        "        f'resource.labels.job_name = \"{self._job_name}\"',\n",
        "        'metric.name = \"dataflow.googleapis.com/job/user_counter\"',\n",
        "        f'metric.labels.metric_name = \"{self._metric_name}\"',\n",
        "    ]\n",
        "    return '({})'.format(' AND '.join(conditions))\n",
        "\n",
        "  def get_latest_value(self):\n",
        "    end_seconds = int(time.time())\n",
        "    start_seconds = 1\n",
        "    interval = monitoring_v3.TimeInterval({\n",
        "        'start_time': { 'seconds': start_seconds},\n",
        "        'end_time': { 'seconds': end_seconds }\n",
        "    })\n",
        "    request = {\n",
        "        'name': f'projects/{self._project_id}',\n",
        "        'filter': self._filter,\n",
        "        'interval': interval,\n",
        "        'view': monitoring_v3.ListTimeSeriesRequest.TimeSeriesView.FULL\n",
        "    }\n",
        "    results = self._client.list_time_series(request)\n",
        "    if (len(results._response.time_series) == 0 or\n",
        "        len(results._response.time_series[0].points) == 0):\n",
        "      return None, None\n",
        "\n",
        "    latest_point = results._response.time_series[0].points[0]\n",
        "    return latest_point.interval.end_time, latest_point.value.double_value\n",
        "\n",
        "class ProgressBar:\n",
        "  def __init__(self, max):\n",
        "    self._display = display(self.get_html(0, max), display_id=True)\n",
        "\n",
        "  def get_html(self, value, max):\n",
        "    return HTML(f'Num generated examples: {value}/{max}<progress value=\"{value}\" max=\"{max}\" style=\"width: 100%\">{value}</progress>')\n",
        "\n",
        "  def update(self, num_examples, max):\n",
        "    self._display.update(self.get_html(num_examples, max))\n",
        "\n",
        "def parse_dataflow_job_creation_params(param_str: str):\n",
        "  params_dict = {}\n",
        "  lines = [line.strip() for line in param_str.split('\\r\\n')]\n",
        "  for line in lines:\n",
        "    if not line:\n",
        "      continue\n",
        "    pieces = line.strip().split(':')\n",
        "    key = pieces[0].strip()\n",
        "    value = pieces[1].strip().strip(\"'\")\n",
        "    params_dict[key] = value\n",
        "  return params_dict\n",
        "\n",
        "def run_example_generation(generate_examples_args, pretty_output=True):\n",
        "  if not pretty_output:\n",
        "    launch_pexpect_process(\n",
        "        pathsys_venv, \n",
        "        pathsys_skai,\n",
        "        'generate_examples_main.py',\n",
        "        generate_examples_args,\n",
        "        use_pexpect=False)\n",
        "    return\n",
        "\n",
        "  progress_bar = ProgressBar(1)\n",
        "\n",
        "  child = launch_pexpect_process(\n",
        "      pathsys_venv, \n",
        "      pathsys_skai,\n",
        "      'generate_examples_main.py',\n",
        "      generate_examples_args,\n",
        "      use_pexpect=True)\n",
        "\n",
        "  JOB_CREATION_PATTERN = 'Create job: <Job(.*clientRequestId:.*)>'\n",
        "  BUILDINGS_MATCHED_PATTERN = 'Found ([0-9]+) buildings in area of interest.'\n",
        "\n",
        "  num_buildings = 1\n",
        "  while child.isalive():\n",
        "    i = child.expect([BUILDINGS_MATCHED_PATTERN, JOB_CREATION_PATTERN, pexpect.EOF], timeout=600)\n",
        "    if i == 0:\n",
        "      num_buildings = int(child.match.group(1))\n",
        "      print(f'Found {num_buildings} buildings in area of interest.')\n",
        "      progress_bar.update(0, num_buildings)\n",
        "    elif i == 1:\n",
        "      job_params = parse_dataflow_job_creation_params(child.match.group(1).decode())\n",
        "      job_name = job_params['name']\n",
        "      job_id = job_params['id']\n",
        "      job_location = job_params['location']\n",
        "      job_project = job_params['projectId']\n",
        "      job_status_pattern = f'Job {job_id} is in state JOB_STATE_([A-Z]+)'\n",
        "      print(f'Detailed monitoring page: https://console.cloud.google.com/dataflow/jobs/{job_location}/{job_id}?project={job_project}')\n",
        "      break\n",
        "    else:\n",
        "      print(child.before.decode())\n",
        "      child.close()\n",
        "      raise Exception('Job terminated unexpectedly.')\n",
        "\n",
        "  generated_examples_metric = DataflowMetricFetcher(job_project, job_name, 'generated_examples_count')\n",
        "  rejected_examples_metric = DataflowMetricFetcher(job_project, job_name, 'rejected_examples_count')\n",
        "\n",
        "  job_state = None\n",
        "  while child.isalive():\n",
        "    i = child.expect([job_status_pattern, pexpect.TIMEOUT, pexpect.EOF], timeout=15)\n",
        "    if i == 0:\n",
        "      job_state = child.match.group(1).decode()\n",
        "      print(f'Dataflow job state: {job_state}')\n",
        "    elif i == 1 or i == 2:\n",
        "      if job_state == 'RUNNING':\n",
        "        examples_processed = 0\n",
        "        t, v = generated_examples_metric.get_latest_value()\n",
        "        if t:\n",
        "          examples_processed += int(v)\n",
        "        t, v = rejected_examples_metric.get_latest_value()\n",
        "        if t:\n",
        "          examples_processed += int(v)\n",
        "        progress_bar.update(examples_processed, num_buildings)\n",
        "      if i == 2:\n",
        "        child.close()\n",
        "        break\n",
        "\n",
        "## COMMAND RUN\n",
        "generate_examples_args = {\n",
        "    'cloud_project': GCP_PROJECT,\n",
        "    'cloud_region': GCP_LOCATION,\n",
        "    'before_image_path': f'gs://{pathgcp_imagebefore}',\n",
        "    'after_image_path': f'gs://{pathgcp_imageafter}',\n",
        "    'aoi_path': f'gs://{pathgcp_aoi}',\n",
        "    'output_dir': f'gs://{pathgcp_outputdir}',\n",
        "    'buildings_method': BUILDING_DETECTION_METHOD,\n",
        "    'buildings_file': BUILDINGS_CSV,\n",
        "    'worker_service_account': 'skai-colab@skai-2022.iam.gserviceaccount.com',\n",
        "    'use_dataflow': 'true',\n",
        "    'num_labeling_examples': 1000\n",
        "}\n",
        "\n",
        "run_example_generation(generate_examples_args)\n",
        "\n",
        "def count_tfrecord(path):\n",
        "  pre_images = []\n",
        "  post_images = []\n",
        "  labels = []\n",
        "  labels_split=[]\n",
        "  total_example_num=len(list(tf.data.TFRecordDataset(path)))\n",
        "  return total_example_num\n",
        "\n",
        "total_example_counter=0\n",
        "for k in range(20):\n",
        "  file_directory='unlabeled/unlabeled-000{:02d}-of-00020.tfrecord'.format(k)\n",
        "  tfrecord_path = os.path.join('gs://',pathgcp_examples,file_directory)\n",
        "  total_example_counter+=count_tfrecord(tfrecord_path)\n",
        "\n",
        "print('{} building examples were extracted in total from the Area Of Interest'.format(total_example_counter))"
      ],
      "metadata": {
        "id": "q0GMnQAGtbCt",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Second, we create the labeling tasks for the labelers. This task should take about 15 minutes.\n",
        "At the end of this step you and each labelers will receive an email with the instruction on how to perform the labeling task."
      ],
      "metadata": {
        "id": "rcx20opc76D5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Create Labeling Task\n",
        "\n",
        "class LabelingJob:\n",
        "  def __init__(self, endpoint, project, location, labeling_job):\n",
        "    self._endpoint = endpoint\n",
        "    self._project = project\n",
        "    self._location = location\n",
        "    self._labeling_job = labeling_job\n",
        "    self._access_token = self.get_access_token()\n",
        "\n",
        "    job_info = self.get_info()\n",
        "    self._dataset = job_info['datasets'][0]\n",
        "\n",
        "    assert len(job_info['specialistPools']) == 1\n",
        "    # Has the format projects/{project_id}/locations/{location}/specialistPools/{pool_id}\n",
        "    parts = job_info['specialistPools'][0].split('/')\n",
        "    assert len(parts) == 6\n",
        "    assert parts[4] == 'specialistPools'\n",
        "    self._pool_id = parts[5]\n",
        "    \n",
        "  def get_access_token(self):\n",
        "    return subprocess.check_output('gcloud auth print-access-token'.split()).decode().rstrip('.\\r\\n')\n",
        "  \n",
        "  def get_header(self):\n",
        "    return {\n",
        "      'Authorization': f'Bearer {self._access_token}',\n",
        "      'Content-Type': 'application/json',\n",
        "    }\n",
        "  \n",
        "  def get_info(self):\n",
        "    '''Return the percentage of data items labeled.\n",
        "\n",
        "    Warning: There is a long lag between when items are labeled and when this\n",
        "    value is updated.\n",
        "    '''\n",
        "    parent = f'projects/{self._project}/locations/{self._location}/dataLabelingJobs/{self._labeling_job}'\n",
        "    url = f'https://{self._endpoint}/v1/{parent}'\n",
        "    header = self.get_header()\n",
        "    r = requests.get(url, headers=header)\n",
        "    if not r.ok:\n",
        "      r.raise_for_status()\n",
        "    return r.json()\n",
        "\n",
        "  def get_completion_percentage(self):\n",
        "    '''Return the percentage of data items labeled.\n",
        "\n",
        "    Warning: There is a long lag between when items are labeled and when this\n",
        "    value is updated.\n",
        "    '''\n",
        "    info = self.get_info()\n",
        "    return info.get('labelingProgress', 0)\n",
        "    \n",
        "  def get_data_items(self):\n",
        "    parent = f'projects/{self._project}/locations/{self._location}/datasets/{self._dataset}/dataItems'\n",
        "    url = f'https://{self._endpoint}/v1/{parent}'\n",
        "    items = []\n",
        "    page_token = None\n",
        "    header = self.get_header()\n",
        "    while True:\n",
        "      if page_token:\n",
        "        r = requests.get(url, headers=header, params={'pageToken': page_token})\n",
        "      else:\n",
        "        r = requests.get(url, headers=header)\n",
        "      if not r.ok:\n",
        "        r.raise_for_status()\n",
        "\n",
        "      result_json = r.json()\n",
        "      items.extend(result_json['dataItems'])\n",
        "      if 'nextPageToken' in result_json:\n",
        "        page_token = result_json['nextPageToken']\n",
        "      else:\n",
        "        break\n",
        "    return items\n",
        "\n",
        "  def get_labels(self, data_item_name):\n",
        "    url = f'https://{self._endpoint}/v1/{data_item_name}/annotations'\n",
        "    header = self.get_header()\n",
        "    r = requests.get(url, headers=header)\n",
        "    if not r.ok:\n",
        "      r.raise_for_status()\n",
        "    json = r.json()\n",
        "    labels = []\n",
        "    if 'annotations' in json:\n",
        "      for a in json['annotations']:\n",
        "        labels.append(a['payload']['displayName'])\n",
        "    return labels\n",
        "\n",
        "  def get_worker_url(self):\n",
        "    '''Returns the URL workers can use to access the labeling interface.\n",
        "\n",
        "    The syntax of the URL was determined by reverse engineering, so there's no\n",
        "    guarantee that it won't change in the future.\n",
        "    '''\n",
        "    location = self._location.replace('-', '_')\n",
        "    return f'https://datacompute.google.com/w/cloudml_data_specialists_{location}_{self._pool_id}'\n",
        "\n",
        "  def get_manager_url(self):\n",
        "    '''Returns the URL managers can use to access the task management interface.\n",
        "\n",
        "    The syntax of the URL was determined by reverse engineering, so there's no\n",
        "    guarantee that it won't change in the future.\n",
        "    '''\n",
        "    location = self._location.replace('-', '_')\n",
        "    return f'https://datacompute.google.com/cm/cloudml_data_specialists_{location}_{self._pool_id}/tasks'\n",
        "\n",
        "def run_labeling_task_creation(create_label_task_args, pretty_output=True):\n",
        "  if not pretty_output:\n",
        "    launch_pexpect_process(\n",
        "        pathsys_venv, pathsys_skai, 'create_cloud_labeling_task.py',\n",
        "        create_label_task_args, False)\n",
        "    return None\n",
        "\n",
        "  child = launch_pexpect_process(\n",
        "      pathsys_venv, pathsys_skai, 'create_cloud_labeling_task.py',\n",
        "      create_label_task_args, True)\n",
        "\n",
        "  DATASET_CREATED_PATTERN = 'ImageDataset created. Resource name: projects/[^/]+/locations/[^/]+/datasets/([0-9]+)'\n",
        "  LABELING_JOB_CREATED_PATTERN = 'Data labeling job created:'\n",
        "\n",
        "  output = b''\n",
        "  try:\n",
        "    while child.isalive():\n",
        "      i = child.expect(\n",
        "          [DATASET_CREATED_PATTERN,\n",
        "           LABELING_JOB_CREATED_PATTERN,\n",
        "           pexpect.EOF,\n",
        "           pexpect.TIMEOUT], timeout=1800)\n",
        "      if isinstance(child.before, bytes):\n",
        "        output += child.before\n",
        "      if isinstance(child.after, bytes):\n",
        "        output += child.after\n",
        "      if i == 0:\n",
        "        dataset_id = child.match.group(1).decode()\n",
        "      elif i == 1:\n",
        "        print('Data labeling job created.')\n",
        "      elif i == 2:\n",
        "        break\n",
        "      else:\n",
        "        raise Exception('Job timed out. Full output:\\n' + output.decode())\n",
        "  finally:\n",
        "    child.close()\n",
        "\n",
        "  return dataset_id\n",
        "\n",
        "## COMMAND RUN\n",
        "\n",
        "timestamp = datetime.datetime.now().strftime('%Y%m%d%H%M%S')\n",
        "GCP_DATASET_NAME = f\"{Author}_label_{timestamp}_{Project_description}\"\n",
        "\n",
        "create_labeling_task_args = {\n",
        "    'cloud_project':GCP_PROJECT,\n",
        "    'cloud_location':GCP_LOCATION_LABELING,\n",
        "    'dataset_name': GCP_DATASET_NAME,\n",
        "    'import_file': f'gs://{pathgcp_importfile}',\n",
        "    'cloud_labeler_emails': GCP_LABELER_EMAIL\n",
        "    }\n",
        "\n",
        "print('Creating data labeling job.')\n",
        "GCP_DATASET_ID = run_labeling_task_creation(create_labeling_task_args)\n",
        "\n",
        "url='https://{}-aiplatform.googleapis.com/v1/projects/{}/locations/{}/dataLabelingJobs'.format(GCP_LOCATION_LABELING,GCP_PROJECT,GCP_LOCATION_LABELING)\n",
        "data = make_gcp_http_request(url)\n",
        "data = list(filter(lambda d: GCP_DATASET_NAME in d['displayName'], data['dataLabelingJobs']))[0]\n",
        "\n",
        "GCP_DATASET_ID = int(data['datasets'][0].split('/')[-1])\n",
        "GCP_DATASET_NAME = data['displayName']\n",
        "GCP_LABELING_JOB= int(data['name'].split('/')[-1])\n",
        "GCP_LABELING_INSTRUCTION= data['instructionUri']\n",
        "\n",
        "print(f'\\nLabeling dataset {GCP_DATASET_NAME} created, with ID {GCP_DATASET_ID}')\n",
        "print(f'\\nData Labeling job {GCP_LABELING_JOB} created')\n",
        "\n",
        "labeling_job = LabelingJob(f'{GCP_LOCATION_LABELING}-aiplatform.googleapis.com', \n",
        "                           GCP_PROJECT, GCP_LOCATION_LABELING, GCP_LABELING_JOB)\n",
        "print('Instruction URL: {}'.format(GCP_LABELING_INSTRUCTION.replace('gs://','https://storage.cloud.google.com/')))\n",
        "print(f'Worker URL: {labeling_job.get_worker_url()}')\n",
        "print(f'Manager URL: {labeling_job.get_manager_url()}')\n",
        "print(f'Detailed monitoring page: https://console.cloud.google.com/vertex-ai/locations/{GCP_LOCATION_LABELING}/labeling-tasks/{GCP_LABELING_JOB}?project={GCP_PROJECT}')"
      ],
      "metadata": {
        "id": "SGxz7nT7HgjW",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As a manager of the task, you can track the labeling progress by running this script below and see how many labels were created or view the detailed monitoring page. For good quality we recommend having about 200 building labels from the damaged/destroyed and undamaged categories."
      ],
      "metadata": {
        "id": "v98N0B3u8WhP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Monitor Labeling Task\n",
        "labeling_job = LabelingJob(f'{GCP_LOCATION_LABELING}-aiplatform.googleapis.com', \n",
        "                           GCP_PROJECT, GCP_LOCATION_LABELING, GCP_LABELING_JOB)\n",
        "print(f'\\nJob completion percentage: {labeling_job.get_completion_percentage()}%')"
      ],
      "metadata": {
        "id": "qSwaYjKLDry9",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ulMjwGaex8LK"
      },
      "source": [
        "## Create and inspect training and evaluation examples üß©\n",
        "\n",
        "Run this script to assign the labeled images to training and evaluation datasets."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Create training and evaluation datasets\n",
        "\n",
        "def create_labeled_dataset():\n",
        "  child = launch_pexpect_process(pathsys_venv, pathsys_skai, 'create_labeled_dataset.py', {\n",
        "      \"cloud_project\": GCP_PROJECT,\n",
        "      \"cloud_location\": GCP_LOCATION_LABELING,\n",
        "      \"cloud_dataset_id\": GCP_DATASET_ID,\n",
        "      \"cloud_temp_dir\": 'gs://' + pathgcp_temp,\n",
        "      \"examples_pattern\": 'gs://' + pathgcp_unlabeled,\n",
        "      \"train_output_path\": 'gs://' + pathgcp_trainset,\n",
        "      \"test_output_path\": 'gs://' + pathgcp_testset}, True)\n",
        "\n",
        "  print('Creating labeled datasets...')\n",
        "  child.expect(pexpect.EOF, timeout=None)\n",
        "  child.close()\n",
        "  if child.exitstatus != 0:\n",
        "    print('An unexpected error occurred. Output of command was:')\n",
        "    print(child.before.decode())\n",
        "  else:\n",
        "    print('Labeled dataset created.')\n",
        "\n",
        "create_labeled_dataset()"
      ],
      "metadata": {
        "id": "9vSAGvIiTMhq",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sq4JRiN4aVIX"
      },
      "source": [
        "(Optional) You can run the following script to inspect both datasets.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K10K2imWSEcE",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Inspect the training dataset\n",
        "\n",
        "## CLASS DEFINTION\n",
        "\n",
        "#! pip install ipyplot\n",
        "\n",
        "def concat_caption_pilimage(image_before, image_after):\n",
        "  img_before=caption_pilformat(image_before, \"before\")\n",
        "  img_after=caption_pilformat(image_after, \"after\")\n",
        "\n",
        "  w, h=img_before.size\n",
        "\n",
        "  img_concat = Image.new('RGB', (2*w, h), \"white\")\n",
        "  img_concat.paste(img_before, (0, 0))\n",
        "  img_concat.paste(img_after, (w, 0))\n",
        "\n",
        "  return img_concat\n",
        "\n",
        "def caption_pilformat(img_data, caption):\n",
        "  base64_encoded = base64.b64encode(img_data)\n",
        "  im_bytes = base64.b64decode(base64_encoded)\n",
        "  byte_encoded=io.BytesIO(im_bytes)\n",
        "\n",
        "  img=Image.open(byte_encoded)\n",
        "  wd, hg =img.size\n",
        "\n",
        "  img_ = Image.new('RGB', (wd+int(wd/10), hg+int(hg/5)), \"white\")\n",
        "  img_.paste(img, (int(wd/20),int(hg/5)))\n",
        "\n",
        "  wd, hg =img_.size\n",
        "  img_cap = ImageDraw.Draw(img_)\n",
        "  w, h = img_cap.textsize(caption)\n",
        "  img_cap.text(((wd-w)/2,0), caption, fill=(0, 0, 0))\n",
        "\n",
        "  return img_\n",
        "\n",
        "def ipyplot_tfrecord(path, max_examples=None):\n",
        "  pre_images = []\n",
        "  post_images = []\n",
        "  labels = []\n",
        "  labels_split=[]\n",
        "  total_example_num=len(list(tf.data.TFRecordDataset(path)))\n",
        "  print('Number of examples: {}.'.format(total_example_num))\n",
        "\n",
        "  if max_examples==None:\n",
        "    max_examples=total_example_num\n",
        "\n",
        "  for record in tf.data.TFRecordDataset(path):\n",
        "    e = tf.train.Example()\n",
        "    e.ParseFromString(record.numpy())\n",
        "    labels_split.append(e.features.feature['label'].float_list.value[0])\n",
        "    if len(pre_images) < max_examples:\n",
        "      pre_images.append(e.features.feature['pre_image_png'].bytes_list.value[0])\n",
        "      post_images.append(e.features.feature['post_image_png'].bytes_list.value[0])\n",
        "      labels.append(e.features.feature['label'].float_list.value[0])\n",
        "\n",
        "  labels_counter=dict(collections.Counter(labels_split))\n",
        "  map_value = {0: 'Undamaged/bad examples {}/{}'.format(int(len(labels)-sum(labels)),labels_counter[0]),\n",
        "               1: 'Damaged {}/{}'.format(int(sum(labels)),labels_counter[1])}\n",
        "  labels=list((pd.Series(labels)).map(map_value))\n",
        "  \n",
        "  images=[concat_caption_pilimage(pre_images[idx], post_images[idx]) for idx in range(len(pre_images))]\n",
        "\n",
        "  ipyplot.plot_class_tabs(images, labels,max_imgs_per_tab=max_examples, tabs_order=[map_value[1],map_value[0]], img_width=200)\n",
        "\n",
        "  return total_example_num\n",
        "\n",
        "## COMMAND RUN\n",
        "COUNT_TRAIN_LABELED=ipyplot_tfrecord(os.path.join(\"gs://\",pathgcp_trainset),max_examples=40)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FfUy0Ympl4ko",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Inspect the evaluation dataset\n",
        "\n",
        "COUNT_TEST_LABELED=ipyplot_tfrecord(os.path.join(\"gs://\",pathgcp_testset),max_examples=40)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1E0pTjiIeB-w"
      },
      "source": [
        "## Model training, performance evaluation ü§ñ\n",
        "\n",
        "Please run the following script to train the machine learning model and test it using the evaluation dataset (leveraging the examples you labeled)."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The script runs in the background and may take up to 6 hours. You will be able to see the progress on this page and we will also send you an email when this step is done."
      ],
      "metadata": {
        "id": "zeAJKHeSYX6W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Train and evaluate model\n",
        "\n",
        "#ML_METHOD = 'fixmatch'\n",
        "#HP_EPOCH=128\n",
        "#HP_UNLABELEDRATIO=1\n",
        "#HP_BATCH=16\n",
        "\n",
        "#HP_TRAINKIMG=int(HP_BATCH*COUNT_TRAIN_LABELED*HP_EPOCH/1024)\n",
        "\n",
        "def write_train_and_eval_launch_script(**args):\n",
        "  args['hyper_parameters_args']=''\n",
        "\n",
        "  submission_ending = '''\n",
        "export GOOGLE_APPLICATION_CREDENTIALS=/root/service-account-private-key.json\n",
        "\n",
        "source {python_env} ; python {path_skai}/src/launch_vertex_job.py \\\\\n",
        "  --location={cloud_region} \\\\\n",
        "  --project={cloud_project} \\\\\n",
        "  --job_type=train \\\\\n",
        "  --display_name={display_name_train} \\\\\n",
        "  --dataset_name={dataset_name} \\\\\n",
        "  --train_worker_machine_type=n1-highmem-8 \\\\\n",
        "  --train_docker_image_uri_path={train_docker_image_uri_path} \\\\\n",
        "  --service_account={service_account} \\\\\n",
        "  --train_dir={train_dir} \\\\\n",
        "  --train_unlabel_examples={train_unlabel_examples} \\\\\n",
        "  --train_label_examples={train_label_examples} \\\\\n",
        "  --test_examples={test_examples} & \\\\\n",
        "sleep 60 ; python {path_skai}/src/launch_vertex_job.py \\\\\n",
        "  --location={cloud_region} \\\\\n",
        "  --project={cloud_project} \\\\\n",
        "  --job_type=eval \\\\\n",
        "  --display_name={display_name_eval} \\\\\n",
        "  --dataset_name={dataset_name} \\\\\n",
        "  --eval_docker_image_uri_path={eval_docker_image_uri_path} \\\\\n",
        "  --service_account={service_account} \\\\\n",
        "  --train_dir={train_dir} \\\\\n",
        "  --train_unlabel_examples={train_unlabel_examples} \\\\\n",
        "  --train_label_examples={train_label_examples} \\\\\n",
        "  --test_examples={test_examples}'''.format(**args)\n",
        "\n",
        "  with open(args['path_run'], 'w+') as file:\n",
        "    file.write(submission_ending)\n",
        "\n",
        "def metrics(train_label_acc, train_label_auc, test_acc, test_auc):\n",
        "  html = \"\"\"\n",
        "         <h2>Metrics (updated as training progresses):</h2>\n",
        "         <h3>Labeled Training Set</h3> \n",
        "         <p>Accuracy: {train_label_acc}% | AUC: {train_label_auc}</p>\n",
        "         <h3>Test Set</h3>\n",
        "         <p>Accuracy: {test_acc}% | AUC: {test_auc}</p>\n",
        "        \"\"\".format(train_label_acc=train_label_acc, train_label_auc=train_label_auc, test_acc=test_acc, test_auc=test_auc)\n",
        "  return HTML(html)\n",
        "\n",
        "\n",
        "def timestamp_to_datetime(timestamp):\n",
        "  return pd.to_datetime(timestamp)\n",
        "\n",
        "timestamp = datetime.datetime.now().strftime('%Y%m%d%H%M%S')\n",
        "GCP_EXPERIMENT_NAME=f\"{Author}_experiment_{timestamp}_{Project_description}\"\n",
        "GCP_TRAINJOB_NAME=f\"{Author}_train_{timestamp}_{Project_description}\"\n",
        "GCP_EVALJOB_NAME=f\"{Author}_eval_{timestamp}_{Project_description}\"\n",
        "\n",
        "jobgcp_exper=GCP_EXPERIMENT_NAME+'_default'\n",
        "jobgcp_train=GCP_TRAINJOB_NAME+'_default'\n",
        "jobgcp_eval=GCP_EVALJOB_NAME+'_default'\n",
        "\n",
        "pathgcp_exper=os.path.join(pathgcp_models, jobgcp_exper)\n",
        "\n",
        "file_runjob=f'run_jobs_as_child_process_{Author}_{timestamp}_{Project_description}.sh'\n",
        "pathsys_runfile=os.path.join(pathsys_runjobs,file_runjob)\n",
        "\n",
        "generate_script_args={   \n",
        "    'cloud_project':GCP_PROJECT,\n",
        "    'cloud_region':GCP_LOCATION,\n",
        "    'train_docker_image_uri_path':'gcr.io/disaster-assessment/ssl-train-uri',\n",
        "    'eval_docker_image_uri_path':'gcr.io/disaster-assessment/ssl-eval-uri',\n",
        "    'service_account':emailgcp_serviceaccount,\n",
        "    'dataset_name':jobgcp_exper,\n",
        "    'train_dir':f'gs://{pathgcp_exper}',\n",
        "    'train_unlabel_examples':f'gs://{pathgcp_unlabeled}',\n",
        "    'train_label_examples':f'gs://{pathgcp_trainset}',\n",
        "    'test_examples':f'gs://{pathgcp_testset}',\n",
        "    'display_name_train':jobgcp_train,\n",
        "    #'method':ML_METHOD,\n",
        "    #'unlabeled_ratio':HP_UNLABELEDRATIO,\n",
        "    #'batch':HP_BATCH,\n",
        "    #'train_kimg':HP_TRAINKIMG,\n",
        "    'display_name_eval':jobgcp_eval,\n",
        "    'python_env':pathsys_actenv,\n",
        "    'path_skai':pathsys_skai,\n",
        "    'path_run': pathsys_runfile,\n",
        "}\n",
        "\n",
        "write_train_and_eval_launch_script(**generate_script_args)\n",
        "print(f\"\\nYour Custom Training job is :\\n{jobgcp_train}\")\n",
        "print(f\"\\nYour Evaluation job is :\\n{jobgcp_eval}\\n\")\n",
        "\n",
        "\n",
        "# Create the progress bar and metrics displays.\n",
        "progress_display = display(progress(0, 100), display_id=True)\n",
        "metrics_display = None\n",
        "\n",
        "# Store Job IDs of training and evaluation jobs.\n",
        "# Keep track of the timestamp of most recent logs to process only fresher logs. \n",
        "train_job_id = None\n",
        "eval_job_id = None\n",
        "curr_epoch = None\n",
        "total_num_epochs = None\n",
        "train_most_recent_timestamp = pd.Timestamp.utcnow() \n",
        "eval_most_recent_timestamp = pd.Timestamp.utcnow() \n",
        "\n",
        "\n",
        "def update_job_id(job_id):\n",
        "  global train_job_id\n",
        "  global eval_job_id\n",
        "  if train_job_id is None:\n",
        "    train_job_id = job_id\n",
        "    progress_display.update(progress(1, 100))\n",
        "  elif eval_job_id is None:\n",
        "    if job_id != train_job_id:\n",
        "      eval_job_id = job_id \n",
        "\n",
        "\n",
        "# Run the child program.\n",
        "child = pexpect.spawn(f'sh {pathsys_runfile}')\n",
        "while not child.closed:\n",
        "  # Expects 5 different patterns, or EOF (meaning the program terminated).\n",
        "  # Each pattern is a regex and you can use regex match groups \"()\" to extract a\n",
        "  # part of the matched text for later use.\n",
        "  pattern_idx = child.expect([\n",
        "    'I.*] CustomJob created\\. Resource name: .*/([0-9]*)',\n",
        "    'I.*] CustomJob .*/([0-9]*) current state:\\r\\nJobState.JOB_STATE_PENDING',\n",
        "    'I.*] CustomJob .*/([0-9]*) current state:\\r\\nJobState.JOB_STATE_RUNNING',\n",
        "    'I.*] CustomJob run completed.',\n",
        "    pexpect.EOF], timeout=None)\n",
        "  if pattern_idx == 0:  # A job was created, so store its ID.\n",
        "    job_id = child.match.group(1).decode()\n",
        "    update_job_id(job_id)\n",
        "  elif pattern_idx == 1:  # Jobs are pending, so update progress bar. \n",
        "    job_id = child.match.group(1).decode()\n",
        "    if job_id == train_job_id:\n",
        "      progress_display.update(progress(5, 100))\n",
        "  elif pattern_idx == 2:  # Jobs are running, so update progress bar or metrics.\n",
        "    job_id = child.match.group(1).decode()\n",
        "    get_logs_status = os.system(f\"\"\"gcloud logging read 'resource.labels.job_id={job_id} severity=ERROR \"Epoch\"' --format json > /tmp/{job_id}_log\"\"\")\n",
        "    if get_logs_status == 0:\n",
        "      with open(f'/tmp/{job_id}_log', 'r') as log_file:\n",
        "        log_data = json.load(log_file)    \n",
        "        if job_id == train_job_id:\n",
        "          # If training job, then update the progress bar.\n",
        "          curr_epoch = None\n",
        "          for log in log_data:\n",
        "            log_timestamp = timestamp_to_datetime(log[\"timestamp\"])\n",
        "            if log_timestamp < train_most_recent_timestamp:\n",
        "              # If logs have not been refreshed, ignore them.\n",
        "              break\n",
        "            else:\n",
        "              train_most_recent_timestamp = log_timestamp\n",
        "            if log_timestamp == train_most_recent_timestamp:\n",
        "              matches = re.search('Epoch ([0-9]*/[0-9]*):   [0-9]*%', log['jsonPayload']['message'])\n",
        "              if matches:\n",
        "                matches = matches.groups()\n",
        "                log_epoch = int(matches[-1].split('/')[0])\n",
        "                if total_num_epochs is None:\n",
        "                  total_num_epochs = int(matches[-1].split('/')[1])\n",
        "                if curr_epoch is None or log_epoch > curr_epoch:\n",
        "                  # Logs can be received at different times, so check for the\n",
        "                  # highest epoch number logged.\n",
        "                  curr_epoch = log_epoch\n",
        "                  progress_display.update(progress(5 + int(95. * curr_epoch / (total_num_epochs + 1)), 100))\n",
        "                  break\n",
        "        else:\n",
        "          # If evaluation job, then update the metrics display.\n",
        "          train_label_acc, train_label_auc, test_acc, test_auc = None, None, None, None\n",
        "          for log in log_data:\n",
        "            log_timestamp = timestamp_to_datetime(log[\"timestamp\"])\n",
        "            if log_timestamp < eval_most_recent_timestamp:\n",
        "              # If logs have not been refreshed, ignore them.\n",
        "              break\n",
        "            if train_label_acc is None:\n",
        "              train_label_matches = re.search('Train_Label AUC: ([0-9]*\\.[0-9]*), Train_Label Accuracy: ([0-9]*\\.[0-9]*)', log['jsonPayload']['message'])\n",
        "              if train_label_matches:\n",
        "                train_label_auc = train_label_matches.groups()[0]\n",
        "                train_label_acc = train_label_matches.groups()[1]\n",
        "            elif test_acc is None:\n",
        "              test_matches = re.search('Test AUC: ([0-9]*\\.[0-9]*), Test Accuracy: ([0-9]*\\.[0-9]*)', log['jsonPayload']['message'])\n",
        "              if test_matches:\n",
        "                test_auc = test_matches.groups()[0]\n",
        "                test_acc = test_matches.groups()[1]\n",
        "            else:\n",
        "              eval_most_recent_timestamp = log_timestamp\n",
        "              break\n",
        "          if train_label_acc is not None and test_acc is not None:\n",
        "            if metrics_display is None:\n",
        "              metrics_display = display(metrics(0, 0, 0, 0), display_id=True)\n",
        "            else:\n",
        "              metrics_display.update(metrics(train_label_acc, train_label_auc, test_acc, test_auc))          \n",
        "  elif pattern_idx == 3:  # Job completed. Email user a notification.\n",
        "    os.system(f\"\"\"printf 'Subject: Skai Training Complete\\n\\nTraining has completed! Please return to the Colab to visualize results.' | msmtp {EMAIL_MANAGER}\"\"\")\n",
        "    progress_display.update(progress(100, 100))    \n",
        "  else:\n",
        "    child.close()"
      ],
      "metadata": {
        "id": "-thHyb9JYM--",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title View Results in Tensorboard\n",
        "\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir gs://{pathgcp_exper}"
      ],
      "metadata": {
        "cellView": "form",
        "id": "E_RsHs7QXzTE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WXVMbL_PclUV"
      },
      "source": [
        "## Inference prediction üîÆ\n",
        "\n",
        "Run the following script to use the model to create the damage assessment. When it is done you will be shown the summary statistics for the disaster along with a map based visualization of the damaged buildings."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Run Inference\n",
        "\n",
        "# Add custom basemaps to folium.\n",
        "basemaps = {\n",
        "    'Google Maps': folium.TileLayer(\n",
        "        tiles = 'https://mt1.google.com/vt/lyrs=m&x={x}&y={y}&z={z}',\n",
        "        attr = 'Google',\n",
        "        name = 'Google Maps',\n",
        "        overlay = True,\n",
        "        control = True\n",
        "    ),\n",
        "    'Google Satellite': folium.TileLayer(\n",
        "        tiles = 'https://mt1.google.com/vt/lyrs=s&x={x}&y={y}&z={z}',\n",
        "        attr = 'Google',\n",
        "        name = 'Google Satellite',\n",
        "        overlay = True,\n",
        "        control = True\n",
        "    ),\n",
        "    'Google Terrain': folium.TileLayer(\n",
        "        tiles = 'https://mt1.google.com/vt/lyrs=p&x={x}&y={y}&z={z}',\n",
        "        attr = 'Google',\n",
        "        name = 'Google Terrain',\n",
        "        overlay = True,\n",
        "        control = True\n",
        "    ),\n",
        "    'Google Satellite Hybrid': folium.TileLayer(\n",
        "        tiles = 'https://mt1.google.com/vt/lyrs=y&x={x}&y={y}&z={z}',\n",
        "        attr = 'Google',\n",
        "        name = 'Google Satellite',\n",
        "        overlay = True,\n",
        "        control = True\n",
        "    ),\n",
        "    'Esri Satellite': folium.TileLayer(\n",
        "        tiles = 'https://server.arcgisonline.com/ArcGIS/rest/services/World_Imagery/MapServer/tile/{z}/{y}/{x}',\n",
        "        attr = 'Esri',\n",
        "        name = 'Esri Satellite',\n",
        "        overlay = True,\n",
        "        control = True\n",
        "    )\n",
        "}\n",
        "\n",
        "def run_generate_inference_script(**args):\n",
        "\n",
        "  submission_ending = '''\n",
        "export GOOGLE_APPLICATION_CREDENTIALS=/root/service-account-private-key.json\n",
        "\n",
        "source {python_env} ; python {path_skai}/src/launch_vertex_job.py \\\\\n",
        "  --location={cloud_region} \\\\\n",
        "  --project={cloud_project} \\\\\n",
        "  --job_type=eval \\\\\n",
        "  --display_name={display_name_infer} \\\\\n",
        "  --dataset_name={dataset_name} \\\\\n",
        "  --eval_docker_image_uri_path={eval_docker_image_uri_path} \\\\\n",
        "  --service_account={service_account} \\\\\n",
        "  --train_dir={train_dir} \\\\\n",
        "  --test_examples={test_examples} \\\\\n",
        "  --eval_ckpt={eval_model_ckpt} \\\\\n",
        "  --inference_mode=True \\\\\n",
        "  --save_predictions=True'''.format(**args)\n",
        "\n",
        "  with open(args['path_run'], 'w+') as file:\n",
        "    file.write(submission_ending)\n",
        "\n",
        "def create_folium_map(geojson_path):\n",
        "  with open(geojson_path, 'r') as f:\n",
        "    predictions = json.load(f)\n",
        "\n",
        "  damaged_preds = {\n",
        "      'type': predictions['type'],\n",
        "      'features': []\n",
        "  }\n",
        "  undamaged_preds = {\n",
        "      'type': predictions['type'],\n",
        "      'features': []\n",
        "  }\n",
        "  \n",
        "  # Count number of buildings per class.\n",
        "  num_damaged_buildings = 0\n",
        "  num_undamaged_buildings = 0\n",
        "  for feat in predictions['features']:\n",
        "    if feat['properties']['class_1'] >= 0.5:\n",
        "      num_damaged_buildings += 1\n",
        "      damaged_preds['features'].append(feat)\n",
        "    else:\n",
        "      num_undamaged_buildings += 1\n",
        "      undamaged_preds['features'].append(feat)\n",
        "\n",
        "  lat = predictions['features'][0]['properties']['latitude']\n",
        "  lon = predictions['features'][0]['properties']['longitude']\n",
        "  \n",
        "  # Create a folium map object. Location is latitude, longitude.\n",
        "  my_map = folium.Map(location=[lat, lon], zoom_start=16, max_zoom=20)\n",
        "\n",
        "  # Add custom basemaps.\n",
        "  basemaps['Google Maps'].add_to(my_map)\n",
        "  basemaps['Google Satellite Hybrid'].add_to(my_map)\n",
        "\n",
        "  after_image_path = 'gs://'+pathgcp_imageafter\n",
        "  after_map_id_dict = ee.Image.loadGeoTIFF(after_image_path).getMapId()\n",
        "  folium.raster_layers.TileLayer(\n",
        "      tiles=after_map_id_dict['tile_fetcher'].url_format,\n",
        "      attr='COG',\n",
        "      name = 'Post-Disaster Imagery',\n",
        "      overlay = True,\n",
        "      control = True,\n",
        "      max_zoom = 20,\n",
        "    ).add_to(my_map)\n",
        "\n",
        "  # Add predictions.\n",
        "  folium.features.GeoJson(damaged_preds, name='Damaged Predictions', \n",
        "                          style_function=style_function,\n",
        "                          marker=folium.CircleMarker(),\n",
        "                          ).add_to(my_map)\n",
        "  folium.features.GeoJson(undamaged_preds, name='Undamaged Predictions', \n",
        "                          style_function=style_function,\n",
        "                          marker=folium.CircleMarker(),\n",
        "                          ).add_to(my_map)                          \n",
        "\n",
        "  my_map.add_child(folium.LayerControl())\n",
        "\n",
        "  print('Number of Damaged Buildings: ', num_damaged_buildings)\n",
        "  print('Number of Undamaged Buildings: ', num_undamaged_buildings)\n",
        "  print('Total: ', int(num_undamaged_buildings) + int(num_damaged_buildings))\n",
        "  IPython.display.display(my_map)\n",
        "\n",
        "display(Javascript(\"google.colab.output.resizeIframeToContent()\"))\n",
        "\n",
        "# Identify epoch number of last checkpoint.\n",
        "most_recent_epoch_file = os.path.join(f'gs://{pathgcp_exper}', 'checkpoints', 'last_processed_epoch')\n",
        "os.system(f'gsutil cp {most_recent_epoch_file} /tmp/last_processed_epoch')\n",
        "with open('/tmp/last_processed_epoch', 'r') as epoch_f:\n",
        "  epoch_num = epoch_f.read()\n",
        "  epoch = epoch_num.zfill(8)\n",
        "\n",
        "# Create inference script that will be run by child process.\n",
        "timestamp = datetime.datetime.now().strftime('%Y%m%d%H%M%S')\n",
        "GCP_INFERENCE_NAME=f\"{Author}_inference_{timestamp}_{Project_description}\"\n",
        "\n",
        "jobgcp_infer = GCP_INFERENCE_NAME + '_default'\n",
        "\n",
        "file_runjob=f'run_inference_as_child_process_{Author}_{timestamp}_{Project_description}.sh'\n",
        "pathsys_runfile=os.path.join(pathsys_runjobs,file_runjob)\n",
        "\n",
        "generate_script_args={   \n",
        "    'cloud_project':GCP_PROJECT,\n",
        "    'cloud_region':\"europe-west1\",\n",
        "    'eval_docker_image_uri_path':'gcr.io/disaster-assessment/ssl-eval-uri',\n",
        "    'service_account':emailgcp_serviceaccount,\n",
        "    'dataset_name':jobgcp_exper,\n",
        "    'train_dir':'gs://'+pathgcp_exper,\n",
        "    'test_examples':'gs://'+pathgcp_unlabeled,\n",
        "    'display_name_infer':jobgcp_infer,\n",
        "    'eval_model_ckpt': 'gs://'+pathgcp_exper+'/checkpoints/model.ckpt-'+epoch,\n",
        "    'python_env':pathsys_actenv,\n",
        "    'path_skai':pathsys_skai,\n",
        "    'path_run': pathsys_runfile,\n",
        "    }\n",
        "\n",
        "run_generate_inference_script(**generate_script_args)\n",
        "\n",
        "\n",
        "# Prepare credentials for map visualization.\n",
        "service_account = 'skai-colab@skai-2022.iam.gserviceaccount.com'\n",
        "credentials = ee.ServiceAccountCredentials(\n",
        "    service_account, '/root/service-account-private-key.json')\n",
        "ee.Initialize(credentials)\n",
        "# Set style parameters.\n",
        "style_function = lambda x: {\n",
        "  'radius': 10,\n",
        "  'weight': 1,\n",
        "  'fill': True,\n",
        "  'color': '#ff0000' if float(x['properties']['class_1']) >= 0.5 else '#00ff00',\n",
        "  'fillColor': '#ff0000' if float(x['properties']['class_1']) >= 0.5 else '#00ff00',\n",
        "  'fillOpacity': 0.3\n",
        "}\n",
        "\n",
        "# Initialize progress bar.\n",
        "progress_display = display(progress(0, 100), display_id=True)\n",
        "curr_idx = 0\n",
        "map = None\n",
        "\n",
        "# Run the child program.\n",
        "child = pexpect.spawn(f'sh {pathsys_runfile}')\n",
        "while not child.closed:\n",
        "  # Expects 5 different patterns, or EOF (meaning the program terminated).\n",
        "  # Each pattern is a regex and you can use regex match groups \"()\" to extract a\n",
        "  # part of the matched text for later use.\n",
        "  pattern_idx = child.expect([\n",
        "    'CustomJob created\\.',\n",
        "    'JobState\\.JOB_STATE_PENDING\\r\\n',\n",
        "    'JobState\\.JOB_STATE_RUNNING\\r\\n',\n",
        "    'CustomJob run completed\\.',\n",
        "    pexpect.EOF], timeout=None)\n",
        "  if pattern_idx == 0:  # A job was created.\n",
        "    progress_display.update(progress(5, 100))\n",
        "  elif pattern_idx == 1:  # Job Pending.\n",
        "    progress_display.update(progress(10, 100))\n",
        "  elif pattern_idx == 2:  # Job Running.\n",
        "    starting_progress = 20\n",
        "    max_progress = 90\n",
        "    curr_progress = starting_progress + (curr_idx * 2)\n",
        "    if curr_idx == 0:\n",
        "      progress_display.update(progress(starting_progress, 100))\n",
        "    elif curr_progress < max_progress:\n",
        "      # Update while job is running only until progress hits 90.\n",
        "      progress_display.update(progress(curr_progress, 100))\n",
        "    curr_idx += 1\n",
        "  elif pattern_idx == 3:  # Job Completed.\n",
        "    progress_display.update(progress(100, 100))\n",
        "  else:\n",
        "    child.close()\n",
        "\n",
        "preds_file = os.path.join(f'gs://{pathgcp_exper}', 'predictions', f'test_ckpt_{epoch_num}.geojson')\n",
        "os.system(f'gsutil cp {preds_file} /tmp/predictions.geojson')\n",
        "create_folium_map('/tmp/predictions.geojson')\n"
      ],
      "metadata": {
        "id": "22PspX0hKnIX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "2W847Be1uadN"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "WXVMbL_PclUV"
      ],
      "name": "SKAI_2022_Colab_Github.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}